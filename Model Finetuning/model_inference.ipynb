{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jellyfish/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: transformers in /home/jellyfish/.local/lib/python3.10/site-packages (4.37.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/jellyfish/.local/lib/python3.10/site-packages (2.5.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/jellyfish/.local/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: requests in /home/jellyfish/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/jellyfish/.local/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jellyfish/.local/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jellyfish/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/jellyfish/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jellyfish/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jellyfish/.local/lib/python3.10/site-packages (from transformers) (20.9)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: networkx in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: fsspec in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/jellyfish/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jellyfish/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/jellyfish/.local/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jellyfish/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jellyfish/.local/lib/python3.10/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jellyfish/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jellyfish/.local/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jellyfish/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Installing collected packages: huggingface-hub, torch, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.21.4\n",
      "    Uninstalling huggingface-hub-0.21.4:\n",
      "      Successfully uninstalled huggingface-hub-0.21.4\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.0\n",
      "    Uninstalling torch-2.5.0:\n",
      "      Successfully uninstalled torch-2.5.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.37.2\n",
      "    Uninstalling transformers-4.37.2:\n",
      "      Successfully uninstalled transformers-4.37.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xformers 0.0.28.post2 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
      "unsloth 2024.10.7 requires protobuf<4.0.0, but you have protobuf 4.25.5 which is incompatible.\n",
      "unsloth 2024.10.7 requires transformers<4.45.0, but you have transformers 4.46.2 which is incompatible.\n",
      "unsloth-zoo 2024.10.1 requires protobuf<4.0.0, but you have protobuf 4.25.5 which is incompatible.\n",
      "unsloth-zoo 2024.10.1 requires transformers<4.45.0, but you have transformers 4.46.2 which is incompatible.\n",
      "torchvision 0.20.0 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
      "torchaudio 2.2.1 requires torch==2.2.1, but you have torch 2.5.1 which is incompatible.\n",
      "cohere 5.5.4 requires tokenizers<0.16,>=0.15, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.26.2 tokenizers-0.20.3 torch-2.5.1 transformers-4.46.2\n"
     ]
    }
   ],
   "source": [
    "%pip install -U sentencepiece transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:28:49.407107: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-11 11:28:50.178800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-11 11:28:53.439717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39071bda0bc240b794b0b63d0bea2694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec4f2a8fb6a491caa1fcc13a101af89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58bc9bcf73443f69f9ae19921670c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc6206e243140e79f2572e21264c0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c09826368a45c3ab68cc44109095df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6d94dd8961461e8941e84c65e167b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = 'abhinavsarkar/Google-T5-base-Grammatical_Error_Correction-Finetuned-C4-200M-550k'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I know some people who rob the bank.', 'I know some peoples who rob the bank.']\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text\n",
    "text = 'I know some peoples which robb the bank'\n",
    "print(correct_grammar(text, num_return_sequences=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
